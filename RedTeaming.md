# Red Teaming
_Rob Hunter - July 2017_
> _Collector - Builder - Distiller - Breaker - Curator_

### Introduction
Ideally a red team would challenge current assumptions using critical and adversarial thinking that is creative and incorporates the various cultural prospective of those involved to explore alternative decision making paths.  This alternative and independent based decision making process is in stark contrast to the common method of decision making with is typically made from bias based assumptions or instinct when no actual experience may be drawn upon (Longbine, 2008).   History tells of many instances were key decisions were executed in opposition of existing thought, and concepts.  This essay will explore the objectives, key concepts, some history, and the application of Red Teaming in order to provide a clear understanding of the topic.

### Key Concepts
The primary objective of Red Teaming is to challenge an organization’s thinking or systems in order to reduce risk.  Through the application of Red Teaming, the organization will gain a new prospective and awareness in order to preemptively to take steps that mitigate risk or the Red Teaming efforts will deliver deeper insights into the organizations values and though process (Zenko, 2015, p.212-213).  There are many definitions that apply to Red Teaming, but the key concepts used to challenge assumptions and overcome institutional inertia are the use of skeptical yet creative thinking, acting as an adversary to reduce risk, experience, and incorporating various outside perspectives with an emphasis on increased cultural and self-awareness, while remaining objective with a big picture view.

Skeptical yet creative thinking often describes an individual that is not bound by the restraints of hierarchy and allows for open and divergent thinking.  Being creative yet critical requires the individual to almost be a non-conformist to traditional authority models and be dismissive of conventional wisdom.  (Zenko, 2015, p.10)

Acting as an adversary in order to reduce risk allows the Red Team to test a system in ways not anticipated by the architect.  This includes simulating attacks that test known structures, and defenses for normal use cases, as well as unexpected attack vectors on the fringes.  In this way, Red Teaming utilizes attacks, and/or analyzes to seek out unforeseen decision structures, and use cases to uncover unknown vulnerabilities in a system in order to aid in its defense while mitigating risk.

Experience is uncovered only after gaining a perspective on events significant enough to warrant retrospective thought that stemmed from knowledge or sparked a thirst for it.  Red Teaming is best applied from experience, or at least from earned or well-read knowledge.

Incorporating various perspectives while understanding how beliefs and values impact thinking, and decision making is critical to effective Red Team results.  An understanding of our own self-bias as well as the culture, and beliefs of all involved is critical to laying a foundation for a comprehensive analysis of the situation.

The ability to be objective and maintain a big-picture view is what allows the Red Team to not be swept up in the existing tide of bias or domain knowledge surrounding the organization or system.  By maintaining objectivity, the Red Teamer allows for flexibility in influence, decisions, and outcomes.  The ability to remain objective also implies full access to resources and information, since without full access any effort will be biased towards only the data provided.  (Zenko, 2015)

### History
Based upon current definitions, the use of red teaming spans history with its applications being written about as early as the sixth century BC, when Sun Tzu wrote The Art of War based upon his victories in battle.  One of Sun Tzu’s most often quoted lines refers to knowing yourself and your enemy.  Here is the concept in his words: 

> “Know the enemy and know yourself; in a hundred battles you will never be in peril.  When you are ignorant of the enemy but know yourself, your chances of winning or losing are equal.  If ignorant both of your enemy and of yourself, you are certain in every battle to be in peril. (Tzu, 1971, p.84)”

The previous quotes of Sun Tzu can easily be traced to key concepts of Red Teaming such as incorporating various perspectives, and understanding yourself and your bias.

Other accounts in history provide additional support that Red Teaming can provide solutions that are otherwise unknown based upon current thinking.  In 1942, the British engaged in a military campaign in Burma that showed the positive effects of alternative analysis.  Field Marshall Slim learned that by utilizing alternative tactics he could quickly control the battlespace in a counter offensive in 1945 by utilizing the Royal Air Force (RAF) unlike it had never been envisioned.  The RAF were used in traditional air strikes, but most importantly to move troops, establish field hospitals, and well as perform resupply operations.  This “airmindedness” approach allowed the British forces to establish and maintain operations behind the established enemy lines.  (Longbine, 2008)

### Application
The successful application of Red Teaming requires establishing the proper scope, being properly structured, and being embraced by an executive champion and key stakeholders to perform their role without undue influence (Zenko, 2015, p.233).  Typical application of Red Teaming involves, simulating the competition for commercial clients, attacking secure systems for government clients, and anticipating an enemy’s course of action for the military (Ambrose & Ahern, 2008, p.136).

Ideally a Red Team is utilized to challenge assumptions for the system or organization as a whole since no one component operates solely on its own, however the application and scope is situationally dependent.  Overall, Red Teaming tests design, which is often mistaken for Penetration Testing which also (from a cyber security perspective) simulates the actions of an attacker, but only tests the implementation of the design to see if vulnerabilities can be exploited (Peake, 2003).  In many cases Red Teaming will even test the organization’s ability to detect and respond to a simulated attack (Hayes, 2017).  

A Red Team tasked with assessing a commercial organization will likely take a systems-oriented approach which includes the following aspects of security:  cyber, information, operational, supply chain, physical, personnel; as well as business continuity and emergency management (Mateski, 2017).  In this way the structure of the team will be based upon the type of commercial organization being assessed and will include diverse members, since specialists in a particular field may be required to provide subject matter expertise to the team.  For instance, if the organization had a strong physical security stance, a physical penetration specialist would be included on the team, while one that was mostly cloud base would include someone that specialized in virtualization.  

Ultimately the application of Red Teaming will involve actually performing a simulated attack, probing for vulnerabilities, or some by performing an alternative analysis (Zenko, 2015, p.238). 

### Risk & Benefits
While Red Teaming has many applications, it cannot replace the organization’s internal planning and operations, and most of all the exercise should not be repeated until it is necessary.  Red Teaming risks being effective when it remains a never ending process, and or there is not time to implement its recommendations.  Moreover, it’s suggested that the individuals involved in the process being analyzed run the risk of becoming demoralized and mistrustful of Red Teaming efforts.  Risks run both direction, since an absence of Red Teaming can lead to institutional inertia, group think, and complacency.  Finally, if the Red Teaming efforts are not properly communicated, scoped, and supported by an executive champion, and key stakeholders, then all efforts risk being marginalized. (Zenko, 2015, p.20-21)

The benefit of being exposed to Red Teaming, is a new found confidence through study of alternatives.  While some alternatives lead to failure, others uncover areas for improvement, enable honest educated discussions, and offer solutions that would not have been possible without Red Teaming.  (Zenko, 2015, p.16)

### Conclusion
While Red Teaming is a learned skill-set that should be carefully applied, it’s likely not for everyone.  The website that started me down this road is redteams.net, which posted an article about physical entry, and hacking that involved rappelling, lock-picking, and network exploits.  I believe that it takes a well-rounded individual to be a Red Teamer, the following does a great job of bringing home the point: 

> “A Red Teamer is simply a person that can think like the adversary, find the way around things and test/push the limits of security, plans, policies and assumptions. Simple.

A lot of different people can fit in here: hackers, physical security experts, physicists, phycologists, Law Enforcement professionals, military personnel, teachers... Anyone can fit here. It's not what you do, but your mindset (RedTeams, n.d.)”  



 
### References:
* Ambrose, F., Ahern, B.  (2008).  Retrieved from http://www.redteamjournal.com/papers/U_White_Paper-Anticipating_Rare_Events_Nov2008rev.pdf.
* Craig, S.  (2007).  Reflections from a RED TEAM LEADER.  Retrieved from http://www.au.af.mil/au/awc/awcgate/milreview/craig.pdf.
* Hayes, K.  (2016).  Retrieved from https://community.rapid7.com/community/infosec/blog/2016/06/23/penetration-testing-vs-red-teaming-the-age-old-debate-of-pirates-vs-ninja-continues.
* Longbine, D.  (2008).  Red Teaming: Past and Present, A Monograph.  Retrieved from https://nu.blackboard.com/bbcswebdav/pid-4150659-dt-content-rid-1609353_1/institution/National%20University/School%20of%20Engineering%20and%20Computing%20%28SOEC%29/Cyber%20Security%20Technology%20%28CYB%29/CYB%20633/Week%20One/Readings/Red%20Teaming-Past%20and%20Present.pdf.
* Mateski, M.  (2017).  Retrieved from http://redteamjournal.com/2017/07/httpredteamjournal-com201707strategic-red-teaming-the-job-description/.
* Peake, C.  (2003).  Retrieved from https://www.sans.org/reading-room/whitepapers/auditing/red-teaming-art-ethical-hacking-1272.
* RedTeams.  (n.d.).  Retrieved from https://redteams.net/?offset=1463613138217.
* Tzu, S.  (1971).  The Art of War.  New York, New York; Oxford University Press.
* Zenko, M.  (2015).  Red Team: How to Succeed by Thinking Like The Enemy.  New York, New York; Basic Books, A Member of the Perseus Books Group.


